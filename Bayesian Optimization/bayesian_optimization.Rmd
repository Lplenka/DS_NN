---
title: "Bayesian Optimization"
output: html_notebook
---

# Bayesian Hyperparameter Optimization for Time Series

## Introduction
Hyperparameter tuning is a computationally intensive process, even when you have domain knowledge of a particular algorithm’s parameters. Bayesian Optimization is a method that uses a small amount of random grid search evaluation data to choose parameters that often far outperform those selected by intensive grid search. Below, I will outline how to use Bayesian Optimization to tune the hyperparameters in Facebook’s Prophet time-series package. We will be predicting property crime in a specific Chicago police districts at a weekly level.

## Pulling Open Data
First, we pull all burglaries in the city since 2001. We access Chicago’s Open Data Portal through the RSocrata API interface. Then we will aggregate to counts by week and district.


```{r}
library(RSocrata)
library(dplyr)
library(lubridate)
```


```{r}
fbi_code = "'05'"
url = sprintf("https://data.cityofchicago.org/resource/6zsd-86xi.json?$select=*&$where=fbi_code=%s", fbi_code)
burglaries = read.socrata(url)

#Converts date format from "%Y-%m-%d %H:%M:%S" to "%Y-%m-%d"
burglaries$date_clean = as.Date(as.POSIXct(burglaries$date, format = '%Y-%m-%d %H:%M:%S'))


#Creates a column "week" containing first date of the week which the column "date_clean" belongs
burglaries$week= as.Date(cut(burglaries$date_clean, 'week'))

```


```{r}
#Find the burglary counts per district per week
burglary_counts       = burglaries %>%
  group_by(district, week) %>%
  summarise(COUNTCRIMES = length(district))

```

```{r}
head(burglary_counts)
```

## Cross Validation Framework

Below, I highlight the cross validation framework for choosing the best parameters. We will use Hyndman’s Evaluation on a rolling forecasting origin method, illustrated by the graph below. The blue points represent training periods, and the red point represents validation periods. We roll over several validation periods, adding a new training period each time, and averaging our evaluation metric across all of the validation periods. 

#![Blue observations form the training sets, and the red observations form the test sets](https://otexts.com/fpp2/fpp_files/figure-html/cv1-1.png){width=1200px}

## Initial Grid Search
Below we set up the initial random grid for cross validation. We select random samples of parameter values that we believe to be in the correct range. These are then tested using our cross-validation format and the error metric – in this case Mean Average Percent Error (MAPE) – is calculated across the past six weeks.

```{r}
library(prophet)
### rolling cv period
set.seed(12345)
num_weeks   = 6
dist        = '004'
origin_week = as.Date(cut(Sys.Date(), 'week')) - days(7*num_weeks) 
end_week    = as.Date(cut(Sys.Date(), 'week')) - days(7) 
date_seq    = seq(origin_week, end_week, by = '7 days')
```
```{r}
cv_set  = burglary_counts %>%
          ungroup() %>%
          filter(week <= end_week & district == dist) %>%
          select(week, COUNTCRIMES)
```
```{r}
names(cv_set) = c('ds', 'y')
```


```{r}
rand_search_grid =  data.frame( 
  changepoint_prior_scale = sort(runif(10, 0.01, 0.1)),
  seasonality_prior_scale = c(sort(sample(c(runif(5, 0.01, 0.05), runif(5, 1, 10)), 5, replace = F)),
                              sort(sample(c(runif(5, 0.01, 0.05), runif(5, 1, 10)), 5, replace = F))),
  n_changepoints          = sample(5:25, 10, replace = F),
  Value                   = rep(0, 10)
)
```

```{r}
  # Search best parameters
for (i in 1:nrow(rand_search_grid)) {
  parameters = rand_search_grid[i, ]
  error = c()
  for (d in date_seq) {
    train = subset(cv_set, ds < d)
    test  = subset(cv_set, ds == d)
    
    m = prophet(train, growth = 'linear',
                 seasonality.prior.scale = parameters$seasonality_prior_scale, 
                 changepoint.prior.scale = parameters$changepoint_prior_scale,
                 n.changepoints = parameters$n_changepoints,
                 weekly.seasonality = F,
                 daily.seasonality = F)
    
    future = make_future_dataframe(m, periods = 1, freq = 'week')
    
    # NOTE: There's a problem in function names with library(caret)
    forecast = predict(m, future)
    forecast$ds = as.Date(forecast$ds)
    
    error_d = forecast::accuracy(forecast[forecast$ds %in% test$ds, 'yhat'], test$y)[ , 'MAPE']
    error = c(error, error_d)
  }
  mean_error = mean(error)
  print(mean_error)
  rand_search_grid$Value[i] = -mean_error
}
```


```{r}
best_cv_value = -1*rand_search_grid[which.max(rand_search_grid$Value),'Value']
rand_search_grid = arrange(rand_search_grid, -Value)
head(rand_search_grid)
```

## Bayesian Hyperparamter Optimization
Now we implement Bayesian Hyperparameter Optimization through the rBayesianOptimization package. This will take as input: a function that returns a cross-validation metric TO BE MAXIMIZED (thus, the negative of MAPE) for the specific model; the initial tuning grid with the last column being named ‘Value’; and the ‘search bounds’ - or the entire feasible range for the parameters being tuned. To choose these search bounds, we add 20% to both ends of the values we sampled in grid search.

```{r}
library(rBayesianOptimization)
library(ggplot2)
```

```{r}
prophet_fit_bayes = function(changepoint_prior_scale, seasonality_prior_scale, n_changepoints) {
    
    error = c()
    for (d in date_seq) {
      train = subset(cv_set, ds < d)
      test  = subset(cv_set, ds == d)
    
      m = prophet(train, growth = 'linear',
                 seasonality.prior.scale = seasonality_prior_scale, 
                 changepoint.prior.scale = changepoint_prior_scale,
                 n.changepoints = n_changepoints,
                 weekly.seasonality = F,
                 daily.seasonality = F)
    
      future = make_future_dataframe(m, periods = 1, freq = 'week')
    
    # NOTE: There's a problem in function names with library(caret)
      forecast = predict(m, future)
      forecast$ds = as.Date(forecast$ds)
    
      error_d = forecast::accuracy(forecast[forecast$ds %in% test$ds, 'yhat'], test$y)[ , 'MAPE']
      error = c(error, error_d)
  }
  
    ## The function wants to _maximize_ the outcome so we return 
    ## the negative of the resampled MAPE value. `Pred` can be used
    ## to return predicted values but we'll avoid that and use zero
    list(Score = -mean(error), Pred = 0)
}
  
changepoint_bounds    = range(rand_search_grid$changepoint_prior_scale)
n_changepoint_bounds  = as.integer(range(rand_search_grid$n_changepoints))
seasonality_bounds    = range(rand_search_grid$seasonality_prior_scale + 0.0001)

bayesian_search_bounds = list(changepoint_prior_scale = changepoint_bounds,
                              seasonality_prior_scale = seasonality_bounds,
                              n_changepoints = as.integer(n_changepoint_bounds))



ba_search = BayesianOptimization(prophet_fit_bayes,
                                    bounds = bayesian_search_bounds,
                                    init_grid_dt = rand_search_grid, 
                                    init_points = 0, 
                                    n_iter = 15,
                                    acq = 'ucb', 
                                    kappa = 1, 
                                    eps = 0,
                                    verbose = TRUE)
```

```{r}
best_params_ba  = c(ba_search$Best_Par, Value = -1*ba_search$Best_Value)



m = prophet(cv_set, growth = 'linear',
                 seasonality.prior.scale = best_params_ba[['seasonality_prior_scale']], 
                 changepoint.prior.scale = best_params_ba[['changepoint_prior_scale']],
                 n.changepoints = best_params_ba[['n_changepoints']], weekly.seasonality=TRUE, daily.seasonality=TRUE)
```

```{r}
future = make_future_dataframe(m, periods = 1, freq = 'week')
    
    # NOTE: There's a problem in function names with library(caret)
forecast = predict(m, future)
forecast$ds = as.Date(forecast$ds)

p = ggplot() + 
    geom_point(data = subset(cv_set, ds >= origin_week - days(7*52)), aes(x = as.Date(ds), y = y), size = 1) +
    geom_line(data = subset(forecast, ds >= origin_week - days(7*52)), aes(x = as.Date(ds), y = yhat), color = "#0072B2", size = 1) +
    geom_ribbon(data = subset(forecast, ds >= origin_week - days(7*52)), aes(x = as.Date(ds), ymin = yhat_lower, ymax = yhat_upper), fill = "#0072B2", alpha = 0.3) +
    geom_point(data = test, aes(x = as.Date(ds), y = y), size = 1, color = '#4daf4a') 

p
```

```{r}

print(paste('Did district', dist, 'experience a spike in burglaries in the past week?'))
```

```{r}
#Did district 004 experience a spike in burglaries in the past week?"

subset(cv_set, ds == end_week)$y - subset(forecast, ds == end_week)$yhat_upper > 0
```

# Conclusions
```{r}
print('Improvement in MAPE over cross-validation using Bayesian Optimization')
print(paste(round(best_cv_value - best_params_ba[['Value']], 4), 'percentage points'))
```

