{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "# Parameters\n",
    "learning_rate = 0.001\n",
    "training_epochs = 5\n",
    "batch_size = 600\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tf.keras.layers.ReLU()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
      "11493376/11490434 [==============================] - 1s 0us/step\n"
     ]
    }
   ],
   "source": [
    "# Import MNIST data\n",
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
    "\n",
    "train_dataset = (\n",
    "    tf.data.Dataset.from_tensor_slices((tf.reshape(x_train, [-1, 784]), y_train))\n",
    "    .batch(batch_size)\n",
    "    .shuffle(1000)\n",
    ")\n",
    "\n",
    "train_dataset = (\n",
    "    train_dataset.map(lambda x, y:\n",
    "                      (tf.divide(tf.cast(x, tf.float32), 255.0),\n",
    "                       tf.reshape(tf.one_hot(y, 10), (-1, 10))))\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 28, 28)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7fcf901a1748>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(x_train[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Set model weights\n",
    "W = tf.Variable(tf.zeros([784, 10]))\n",
    "b = tf.Variable(tf.zeros([10]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct model\n",
    "model = lambda x: tf.nn.softmax(tf.matmul(x, W) + b) # Softmax\n",
    "# Minimize error using cross entropy\n",
    "compute_loss = lambda true, pred: tf.reduce_mean(tf.reduce_sum(tf.losses.binary_crossentropy(true, pred), axis=-1))\n",
    "# caculate accuracy\n",
    "compute_accuracy = lambda true, pred: tf.reduce_mean(tf.keras.metrics.categorical_accuracy(true, pred))\n",
    "# Gradient Descent\n",
    "optimizer = tf.optimizers.Adam(learning_rate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset label tf.Tensor([0. 0. 0. 1. 0. 0. 0. 0. 0. 0.], shape=(10,), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "for i, (x_, y_) in enumerate(train_dataset):\n",
    "    #print(\"dataset features\",x_[0])\n",
    "    sample = x_[0]\n",
    "    print(\"dataset label\", y_[0])\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> loss 77.76 acc 0.81\n",
      "=> loss 61.33 acc 0.87\n",
      "=> loss 67.14 acc 0.85\n",
      "=> loss 67.35 acc 0.85\n",
      "=> loss 58.26 acc 0.88\n",
      "=> loss 54.77 acc 0.88\n",
      "=> loss 58.64 acc 0.88\n",
      "=> loss 62.11 acc 0.85\n",
      "=> loss 70.97 acc 0.81\n",
      "=> loss 62.05 acc 0.88\n",
      "=> loss 59.06 acc 0.88\n",
      "=> loss 56.54 acc 0.88\n",
      "=> loss 61.23 acc 0.86\n",
      "=> loss 61.31 acc 0.86\n",
      "=> loss 58.85 acc 0.88\n",
      "=> loss 70.00 acc 0.83\n",
      "=> loss 62.35 acc 0.85\n",
      "=> loss 68.25 acc 0.82\n",
      "=> loss 56.85 acc 0.87\n",
      "=> loss 51.43 acc 0.89\n",
      "=> loss 62.56 acc 0.84\n",
      "=> loss 57.19 acc 0.88\n",
      "=> loss 62.49 acc 0.86\n",
      "=> loss 52.54 acc 0.90\n",
      "=> loss 61.58 acc 0.87\n",
      "=> loss 62.42 acc 0.85\n",
      "=> loss 65.68 acc 0.86\n",
      "=> loss 61.34 acc 0.86\n",
      "=> loss 67.60 acc 0.81\n",
      "=> loss 47.63 acc 0.92\n",
      "=> loss 55.76 acc 0.87\n",
      "=> loss 65.29 acc 0.83\n",
      "=> loss 58.77 acc 0.85\n",
      "=> loss 55.86 acc 0.85\n",
      "=> loss 55.06 acc 0.85\n",
      "=> loss 45.89 acc 0.92\n",
      "=> loss 67.06 acc 0.83\n",
      "=> loss 56.55 acc 0.86\n",
      "=> loss 58.18 acc 0.84\n",
      "=> loss 64.06 acc 0.83\n",
      "=> loss 52.32 acc 0.90\n",
      "=> loss 54.71 acc 0.87\n",
      "=> loss 55.91 acc 0.87\n",
      "=> loss 59.65 acc 0.83\n",
      "=> loss 53.60 acc 0.87\n",
      "=> loss 57.85 acc 0.84\n",
      "=> loss 57.02 acc 0.86\n",
      "=> loss 52.00 acc 0.89\n",
      "=> loss 66.67 acc 0.82\n",
      "=> loss 55.05 acc 0.85\n",
      "=> loss 51.74 acc 0.89\n",
      "=> loss 57.63 acc 0.85\n",
      "=> loss 46.19 acc 0.89\n",
      "=> loss 55.05 acc 0.86\n",
      "=> loss 50.66 acc 0.88\n",
      "=> loss 33.80 acc 0.95\n",
      "=> loss 52.59 acc 0.88\n",
      "=> loss 59.28 acc 0.85\n",
      "=> loss 53.08 acc 0.87\n",
      "=> loss 55.24 acc 0.85\n",
      "=> loss 64.02 acc 0.81\n",
      "=> loss 52.10 acc 0.88\n",
      "=> loss 51.34 acc 0.88\n",
      "=> loss 51.51 acc 0.87\n",
      "=> loss 56.09 acc 0.86\n",
      "=> loss 46.01 acc 0.90\n",
      "=> loss 45.66 acc 0.89\n",
      "=> loss 57.14 acc 0.84\n",
      "=> loss 43.98 acc 0.92\n",
      "=> loss 59.16 acc 0.86\n",
      "=> loss 60.12 acc 0.84\n",
      "=> loss 46.04 acc 0.88\n",
      "=> loss 42.35 acc 0.90\n",
      "=> loss 48.39 acc 0.89\n",
      "=> loss 58.50 acc 0.87\n",
      "=> loss 53.00 acc 0.88\n",
      "=> loss 56.34 acc 0.87\n",
      "=> loss 50.78 acc 0.86\n",
      "=> loss 54.56 acc 0.85\n",
      "=> loss 42.50 acc 0.90\n",
      "=> loss 53.72 acc 0.86\n",
      "=> loss 44.91 acc 0.89\n",
      "=> loss 45.19 acc 0.90\n",
      "=> loss 62.59 acc 0.84\n",
      "=> loss 48.16 acc 0.88\n",
      "=> loss 54.25 acc 0.87\n",
      "=> loss 44.80 acc 0.92\n",
      "=> loss 51.25 acc 0.88\n",
      "=> loss 50.18 acc 0.88\n",
      "=> loss 54.69 acc 0.86\n",
      "=> loss 46.43 acc 0.89\n",
      "=> loss 43.35 acc 0.91\n",
      "=> loss 56.37 acc 0.85\n",
      "=> loss 55.79 acc 0.86\n",
      "=> loss 40.08 acc 0.91\n",
      "=> loss 59.73 acc 0.83\n",
      "=> loss 44.04 acc 0.89\n",
      "=> loss 53.35 acc 0.86\n",
      "=> loss 57.22 acc 0.83\n",
      "=> loss 56.86 acc 0.84\n",
      "=> loss 58.76 acc 0.83\n",
      "=> loss 45.69 acc 0.89\n",
      "=> loss 45.31 acc 0.89\n",
      "=> loss 40.56 acc 0.90\n",
      "=> loss 54.32 acc 0.85\n",
      "=> loss 48.99 acc 0.88\n",
      "=> loss 49.07 acc 0.88\n",
      "=> loss 52.19 acc 0.87\n",
      "=> loss 46.38 acc 0.86\n",
      "=> loss 47.11 acc 0.88\n",
      "=> loss 53.98 acc 0.87\n",
      "=> loss 41.74 acc 0.92\n",
      "=> loss 54.61 acc 0.85\n",
      "=> loss 42.33 acc 0.90\n",
      "=> loss 46.26 acc 0.88\n",
      "=> loss 52.39 acc 0.85\n",
      "=> loss 42.38 acc 0.91\n",
      "=> loss 50.70 acc 0.87\n",
      "=> loss 44.84 acc 0.89\n",
      "=> loss 50.05 acc 0.87\n",
      "=> loss 44.33 acc 0.90\n",
      "=> loss 53.36 acc 0.86\n",
      "=> loss 47.44 acc 0.89\n",
      "=> loss 45.80 acc 0.88\n",
      "=> loss 42.73 acc 0.90\n",
      "=> loss 40.09 acc 0.91\n",
      "=> loss 48.91 acc 0.88\n",
      "=> loss 55.36 acc 0.85\n",
      "=> loss 45.88 acc 0.90\n",
      "=> loss 33.78 acc 0.93\n",
      "=> loss 57.24 acc 0.85\n",
      "=> loss 54.90 acc 0.85\n",
      "=> loss 41.18 acc 0.90\n",
      "=> loss 39.17 acc 0.90\n",
      "=> loss 39.44 acc 0.92\n",
      "=> loss 43.77 acc 0.89\n",
      "=> loss 42.72 acc 0.90\n",
      "=> loss 53.78 acc 0.85\n",
      "=> loss 46.63 acc 0.89\n",
      "=> loss 36.63 acc 0.91\n",
      "=> loss 43.12 acc 0.90\n",
      "=> loss 53.65 acc 0.86\n",
      "=> loss 42.80 acc 0.90\n",
      "=> loss 42.39 acc 0.89\n",
      "=> loss 44.91 acc 0.87\n",
      "=> loss 52.28 acc 0.85\n",
      "=> loss 55.65 acc 0.84\n",
      "=> loss 45.93 acc 0.89\n",
      "=> loss 47.73 acc 0.87\n",
      "=> loss 55.40 acc 0.83\n",
      "=> loss 44.46 acc 0.88\n",
      "=> loss 44.54 acc 0.89\n",
      "=> loss 46.80 acc 0.87\n",
      "=> loss 51.50 acc 0.88\n",
      "=> loss 48.07 acc 0.88\n",
      "=> loss 43.33 acc 0.90\n",
      "=> loss 42.67 acc 0.89\n",
      "=> loss 37.88 acc 0.90\n",
      "=> loss 34.93 acc 0.94\n",
      "=> loss 44.15 acc 0.90\n",
      "=> loss 39.56 acc 0.92\n",
      "=> loss 46.04 acc 0.88\n",
      "=> loss 41.47 acc 0.90\n",
      "=> loss 47.44 acc 0.87\n",
      "=> loss 49.54 acc 0.87\n",
      "=> loss 42.11 acc 0.88\n",
      "=> loss 43.02 acc 0.88\n",
      "=> loss 34.68 acc 0.92\n",
      "=> loss 44.04 acc 0.89\n",
      "=> loss 38.04 acc 0.90\n",
      "=> loss 47.85 acc 0.88\n",
      "=> loss 46.09 acc 0.88\n",
      "=> loss 51.41 acc 0.86\n",
      "=> loss 44.91 acc 0.87\n",
      "=> loss 39.01 acc 0.90\n",
      "=> loss 34.68 acc 0.92\n",
      "=> loss 42.27 acc 0.89\n",
      "=> loss 41.07 acc 0.90\n",
      "=> loss 45.80 acc 0.87\n",
      "=> loss 44.36 acc 0.89\n",
      "=> loss 38.85 acc 0.91\n",
      "=> loss 50.23 acc 0.86\n",
      "=> loss 38.17 acc 0.90\n",
      "=> loss 48.06 acc 0.88\n",
      "=> loss 47.19 acc 0.88\n",
      "=> loss 42.65 acc 0.88\n",
      "=> loss 48.07 acc 0.88\n",
      "=> loss 24.72 acc 0.95\n",
      "=> loss 45.68 acc 0.87\n",
      "=> loss 43.71 acc 0.88\n",
      "=> loss 36.52 acc 0.93\n",
      "=> loss 33.95 acc 0.91\n",
      "=> loss 37.92 acc 0.91\n",
      "=> loss 36.36 acc 0.90\n",
      "=> loss 47.00 acc 0.88\n",
      "=> loss 40.68 acc 0.89\n",
      "=> loss 36.17 acc 0.92\n",
      "=> loss 50.74 acc 0.86\n",
      "=> loss 32.93 acc 0.92\n",
      "=> loss 32.61 acc 0.93\n",
      "=> loss 40.82 acc 0.89\n",
      "=> loss 43.66 acc 0.88\n",
      "=> loss 29.02 acc 0.94\n",
      "=> loss 40.66 acc 0.90\n",
      "=> loss 36.66 acc 0.91\n",
      "=> loss 42.88 acc 0.89\n",
      "=> loss 44.75 acc 0.89\n",
      "=> loss 38.27 acc 0.91\n",
      "=> loss 38.97 acc 0.91\n",
      "=> loss 44.05 acc 0.87\n",
      "=> loss 34.67 acc 0.91\n",
      "=> loss 39.53 acc 0.89\n",
      "=> loss 52.23 acc 0.84\n",
      "=> loss 41.96 acc 0.90\n",
      "=> loss 39.01 acc 0.91\n",
      "=> loss 41.90 acc 0.89\n",
      "=> loss 40.36 acc 0.88\n",
      "=> loss 39.14 acc 0.90\n",
      "=> loss 39.12 acc 0.89\n",
      "=> loss 36.26 acc 0.90\n",
      "=> loss 34.72 acc 0.93\n",
      "=> loss 40.10 acc 0.89\n",
      "=> loss 34.88 acc 0.93\n",
      "=> loss 42.16 acc 0.87\n",
      "=> loss 32.35 acc 0.92\n",
      "=> loss 42.33 acc 0.88\n",
      "=> loss 42.95 acc 0.88\n",
      "=> loss 40.06 acc 0.88\n",
      "=> loss 50.55 acc 0.85\n",
      "=> loss 38.71 acc 0.90\n",
      "=> loss 34.55 acc 0.92\n",
      "=> loss 36.45 acc 0.91\n",
      "=> loss 22.94 acc 0.96\n",
      "=> loss 40.14 acc 0.88\n",
      "=> loss 41.05 acc 0.90\n",
      "=> loss 30.47 acc 0.95\n",
      "=> loss 31.69 acc 0.92\n",
      "=> loss 34.85 acc 0.92\n",
      "=> loss 36.79 acc 0.90\n",
      "=> loss 35.82 acc 0.91\n",
      "=> loss 34.71 acc 0.92\n",
      "=> loss 33.24 acc 0.91\n",
      "=> loss 42.27 acc 0.88\n",
      "=> loss 35.13 acc 0.91\n",
      "=> loss 51.20 acc 0.86\n",
      "=> loss 37.48 acc 0.90\n",
      "=> loss 31.89 acc 0.92\n",
      "=> loss 38.64 acc 0.90\n",
      "=> loss 35.42 acc 0.91\n",
      "=> loss 33.56 acc 0.92\n",
      "=> loss 31.34 acc 0.92\n",
      "=> loss 46.00 acc 0.87\n",
      "=> loss 30.95 acc 0.93\n",
      "=> loss 45.04 acc 0.88\n",
      "=> loss 45.97 acc 0.87\n",
      "=> loss 43.50 acc 0.87\n",
      "=> loss 41.68 acc 0.88\n",
      "=> loss 34.90 acc 0.91\n",
      "=> loss 39.56 acc 0.90\n",
      "=> loss 37.62 acc 0.90\n",
      "=> loss 43.81 acc 0.89\n",
      "=> loss 42.08 acc 0.88\n",
      "=> loss 38.81 acc 0.90\n",
      "=> loss 43.64 acc 0.89\n",
      "=> loss 39.09 acc 0.91\n",
      "=> loss 37.59 acc 0.89\n",
      "=> loss 48.61 acc 0.87\n",
      "=> loss 49.65 acc 0.86\n",
      "=> loss 46.14 acc 0.87\n",
      "=> loss 44.09 acc 0.88\n",
      "=> loss 44.87 acc 0.86\n",
      "=> loss 45.57 acc 0.88\n",
      "=> loss 41.92 acc 0.89\n",
      "=> loss 50.11 acc 0.86\n",
      "=> loss 34.93 acc 0.91\n",
      "=> loss 33.25 acc 0.91\n",
      "=> loss 42.10 acc 0.88\n",
      "=> loss 40.27 acc 0.89\n",
      "=> loss 46.39 acc 0.87\n",
      "=> loss 39.96 acc 0.90\n",
      "=> loss 38.67 acc 0.91\n",
      "=> loss 32.86 acc 0.91\n",
      "=> loss 36.52 acc 0.92\n",
      "=> loss 37.79 acc 0.90\n",
      "=> loss 44.80 acc 0.88\n",
      "=> loss 44.41 acc 0.87\n",
      "=> loss 30.22 acc 0.93\n",
      "=> loss 42.32 acc 0.88\n",
      "=> loss 36.38 acc 0.91\n",
      "=> loss 42.21 acc 0.88\n",
      "=> loss 46.27 acc 0.89\n",
      "=> loss 49.73 acc 0.86\n",
      "=> loss 38.73 acc 0.90\n",
      "=> loss 46.93 acc 0.87\n",
      "=> loss 45.54 acc 0.88\n",
      "=> loss 36.80 acc 0.90\n",
      "=> loss 42.35 acc 0.89\n",
      "=> loss 39.01 acc 0.91\n",
      "=> loss 44.43 acc 0.87\n",
      "=> loss 35.20 acc 0.91\n",
      "=> loss 38.81 acc 0.90\n",
      "=> loss 45.48 acc 0.87\n",
      "=> loss 34.73 acc 0.92\n",
      "=> loss 48.97 acc 0.87\n",
      "=> loss 32.04 acc 0.93\n",
      "=> loss 37.76 acc 0.88\n",
      "=> loss 37.85 acc 0.91\n",
      "=> loss 45.14 acc 0.89\n",
      "=> loss 40.18 acc 0.90\n",
      "=> loss 46.09 acc 0.87\n",
      "=> loss 42.57 acc 0.88\n",
      "=> loss 37.93 acc 0.90\n",
      "=> loss 34.92 acc 0.92\n",
      "=> loss 39.94 acc 0.89\n",
      "=> loss 33.25 acc 0.91\n",
      "=> loss 41.79 acc 0.89\n",
      "=> loss 44.39 acc 0.88\n",
      "=> loss 47.81 acc 0.86\n",
      "=> loss 36.09 acc 0.92\n",
      "=> loss 29.32 acc 0.92\n",
      "=> loss 32.47 acc 0.92\n",
      "=> loss 38.85 acc 0.91\n",
      "=> loss 36.39 acc 0.90\n",
      "=> loss 40.09 acc 0.90\n",
      "=> loss 36.01 acc 0.90\n",
      "=> loss 30.45 acc 0.92\n",
      "=> loss 37.46 acc 0.90\n",
      "=> loss 31.76 acc 0.92\n",
      "=> loss 33.89 acc 0.90\n",
      "=> loss 38.77 acc 0.90\n",
      "=> loss 43.18 acc 0.89\n",
      "=> loss 36.10 acc 0.91\n",
      "=> loss 29.14 acc 0.93\n",
      "=> loss 43.11 acc 0.88\n",
      "=> loss 41.42 acc 0.89\n",
      "=> loss 36.28 acc 0.90\n",
      "=> loss 40.21 acc 0.89\n",
      "=> loss 37.69 acc 0.89\n",
      "=> loss 33.26 acc 0.90\n",
      "=> loss 27.11 acc 0.94\n",
      "=> loss 34.65 acc 0.90\n",
      "=> loss 36.61 acc 0.91\n",
      "=> loss 20.57 acc 0.96\n",
      "=> loss 29.21 acc 0.93\n",
      "=> loss 37.78 acc 0.90\n",
      "=> loss 44.11 acc 0.88\n",
      "=> loss 42.51 acc 0.86\n",
      "=> loss 36.49 acc 0.88\n",
      "=> loss 28.36 acc 0.93\n",
      "=> loss 32.25 acc 0.92\n",
      "=> loss 42.71 acc 0.88\n",
      "=> loss 34.73 acc 0.90\n",
      "=> loss 31.40 acc 0.91\n",
      "=> loss 39.33 acc 0.89\n",
      "=> loss 43.21 acc 0.88\n",
      "=> loss 40.04 acc 0.89\n",
      "=> loss 31.08 acc 0.92\n",
      "=> loss 38.90 acc 0.89\n",
      "=> loss 30.51 acc 0.93\n",
      "=> loss 38.11 acc 0.89\n",
      "=> loss 40.05 acc 0.90\n",
      "=> loss 32.46 acc 0.92\n",
      "=> loss 47.34 acc 0.88\n",
      "=> loss 38.12 acc 0.89\n",
      "=> loss 34.24 acc 0.92\n",
      "=> loss 46.17 acc 0.87\n",
      "=> loss 36.02 acc 0.90\n",
      "=> loss 38.96 acc 0.88\n",
      "=> loss 40.63 acc 0.88\n",
      "=> loss 36.86 acc 0.91\n",
      "=> loss 37.21 acc 0.91\n",
      "=> loss 41.26 acc 0.88\n",
      "=> loss 36.12 acc 0.90\n",
      "=> loss 39.90 acc 0.88\n",
      "=> loss 46.61 acc 0.87\n",
      "=> loss 37.67 acc 0.89\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> loss 35.17 acc 0.91\n",
      "=> loss 35.08 acc 0.91\n",
      "=> loss 28.81 acc 0.92\n",
      "=> loss 43.21 acc 0.87\n",
      "=> loss 48.13 acc 0.87\n",
      "=> loss 31.94 acc 0.92\n",
      "=> loss 39.48 acc 0.89\n",
      "=> loss 35.74 acc 0.91\n",
      "=> loss 41.29 acc 0.89\n",
      "=> loss 35.80 acc 0.92\n",
      "=> loss 27.77 acc 0.93\n",
      "=> loss 30.11 acc 0.93\n",
      "=> loss 30.49 acc 0.92\n",
      "=> loss 34.72 acc 0.90\n",
      "=> loss 33.80 acc 0.91\n",
      "=> loss 32.13 acc 0.93\n",
      "=> loss 33.90 acc 0.92\n",
      "=> loss 34.82 acc 0.91\n",
      "=> loss 29.86 acc 0.93\n",
      "=> loss 47.30 acc 0.87\n",
      "=> loss 30.62 acc 0.92\n",
      "=> loss 32.99 acc 0.91\n",
      "=> loss 31.03 acc 0.92\n",
      "=> loss 23.04 acc 0.94\n",
      "=> loss 31.59 acc 0.92\n",
      "=> loss 30.28 acc 0.92\n",
      "=> loss 33.60 acc 0.91\n",
      "=> loss 38.03 acc 0.88\n",
      "=> loss 37.19 acc 0.90\n",
      "=> loss 41.50 acc 0.88\n",
      "=> loss 37.43 acc 0.90\n",
      "=> loss 40.36 acc 0.89\n",
      "=> loss 42.65 acc 0.88\n",
      "=> loss 46.26 acc 0.86\n",
      "=> loss 37.99 acc 0.89\n",
      "=> loss 36.44 acc 0.90\n",
      "=> loss 28.24 acc 0.93\n",
      "=> loss 31.62 acc 0.92\n",
      "=> loss 45.32 acc 0.87\n",
      "=> loss 30.93 acc 0.91\n",
      "=> loss 40.62 acc 0.89\n",
      "=> loss 27.31 acc 0.93\n",
      "=> loss 32.65 acc 0.92\n",
      "=> loss 19.65 acc 0.96\n",
      "=> loss 36.98 acc 0.89\n",
      "=> loss 37.71 acc 0.89\n",
      "=> loss 40.09 acc 0.89\n",
      "=> loss 36.65 acc 0.90\n",
      "=> loss 32.29 acc 0.93\n",
      "=> loss 32.45 acc 0.92\n",
      "=> loss 37.66 acc 0.90\n",
      "=> loss 41.92 acc 0.88\n",
      "=> loss 34.86 acc 0.90\n",
      "=> loss 38.39 acc 0.90\n",
      "=> loss 34.04 acc 0.90\n",
      "=> loss 46.98 acc 0.88\n",
      "=> loss 33.69 acc 0.92\n",
      "=> loss 33.92 acc 0.91\n",
      "=> loss 28.85 acc 0.93\n",
      "=> loss 27.47 acc 0.94\n",
      "=> loss 35.82 acc 0.92\n",
      "=> loss 39.06 acc 0.89\n",
      "=> loss 34.92 acc 0.90\n",
      "=> loss 46.41 acc 0.87\n",
      "=> loss 32.17 acc 0.92\n",
      "=> loss 30.63 acc 0.93\n",
      "=> loss 35.92 acc 0.90\n",
      "=> loss 29.17 acc 0.92\n",
      "=> loss 38.39 acc 0.89\n",
      "=> loss 27.25 acc 0.93\n",
      "=> loss 28.63 acc 0.92\n",
      "=> loss 36.84 acc 0.90\n",
      "=> loss 40.78 acc 0.90\n",
      "=> loss 26.82 acc 0.93\n",
      "=> loss 34.47 acc 0.92\n",
      "=> loss 47.13 acc 0.87\n",
      "=> loss 36.90 acc 0.91\n",
      "=> loss 41.22 acc 0.89\n",
      "=> loss 24.90 acc 0.95\n",
      "=> loss 35.04 acc 0.90\n",
      "=> loss 29.30 acc 0.92\n",
      "=> loss 32.29 acc 0.91\n",
      "=> loss 33.44 acc 0.90\n",
      "=> loss 35.47 acc 0.91\n",
      "=> loss 28.58 acc 0.92\n",
      "=> loss 30.20 acc 0.92\n",
      "=> loss 34.77 acc 0.91\n",
      "=> loss 45.27 acc 0.87\n",
      "=> loss 34.90 acc 0.90\n",
      "=> loss 30.43 acc 0.92\n",
      "=> loss 26.36 acc 0.93\n",
      "=> loss 34.23 acc 0.91\n",
      "=> loss 34.94 acc 0.89\n",
      "=> loss 30.83 acc 0.91\n",
      "=> loss 33.71 acc 0.89\n",
      "=> loss 34.45 acc 0.90\n",
      "=> loss 28.54 acc 0.92\n",
      "=> loss 42.39 acc 0.89\n",
      "=> loss 38.08 acc 0.89\n",
      "=> loss 40.38 acc 0.90\n",
      "=> loss 29.74 acc 0.93\n",
      "=> loss 28.10 acc 0.94\n",
      "=> loss 33.55 acc 0.90\n",
      "=> loss 21.73 acc 0.94\n",
      "=> loss 32.81 acc 0.90\n",
      "=> loss 40.24 acc 0.89\n",
      "=> loss 44.42 acc 0.88\n",
      "=> loss 35.12 acc 0.91\n",
      "=> loss 36.03 acc 0.90\n",
      "=> loss 38.76 acc 0.90\n",
      "=> loss 33.54 acc 0.92\n",
      "=> loss 33.45 acc 0.92\n",
      "=> loss 29.50 acc 0.92\n",
      "=> loss 41.33 acc 0.89\n",
      "=> loss 28.25 acc 0.94\n",
      "=> loss 33.76 acc 0.91\n",
      "=> loss 42.96 acc 0.87\n",
      "=> loss 31.52 acc 0.92\n",
      "=> loss 39.78 acc 0.87\n",
      "=> loss 33.45 acc 0.91\n",
      "=> loss 31.50 acc 0.92\n",
      "=> loss 37.20 acc 0.90\n",
      "=> loss 39.42 acc 0.89\n",
      "=> loss 28.75 acc 0.92\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(training_epochs):\n",
    "    for i, (x_, y_) in enumerate(train_dataset):\n",
    "        with tf.GradientTape() as tape:\n",
    "            pred = model(x_)\n",
    "            loss = compute_loss(y_, pred)\n",
    "        acc = compute_accuracy(y_, pred)\n",
    "        grads = tape.gradient(loss, [W, b])\n",
    "        optimizer.apply_gradients(zip(grads, [W, b]))\n",
    "        print(\"=> loss %.2f acc %.2f\" %(loss.numpy(), acc.numpy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
